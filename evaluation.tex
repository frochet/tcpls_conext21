% !TEX root = ./paper.tex

In this section, we evaluate \tcpls with several experiments.
First, we evaluate the raw performance of our \tcpls prototype in 
Section~\ref{sec:perf}. Then we report how \tcpls
interacts with middleboxes in a controlled environment in 
Section~\ref{sec:middlebox}. We then emulate, using
Mininet~\cite{handigol2012reproducible}, more complex scenarios involving 
Failover (Sec.~\ref{sec:eval_failover}), Application Connection Migration (Sec.
~\ref{sec:app-migration}) and bandwidth aggregation~\ref{sec:bwaggr}.

\todo{State against which protocol we compare tcpls and outline the global 
results.}

For the two first experiments of 
our \tcpls prototype (Sec.~\ref{sec:perf} and~\ref{sec:middlebox}), we use the 
testbed depicted
in Fig.~\ref{fig:perf_testbed}. It consists of three servers equipped
with Intel Xeon CPU E5-2630 2.40GHz, 16~GB RAM, running
Debian with Linux 5.9 and 5.7 kernels. Two of these machines are used as,
respectively, Client and Server, while the third one is used as a router or a
middlebox, depending on the scenario. Each machine is equipped with an Intel
XL710 2x40Gbps NIC. A single \tcp connection can saturate the 40~Gbps path when 
enabling Jumbo frames. With a 1500 bytes MTU, a single \tcp connection reaches 
22~Gbps. For all measurements, with all implementations, we use a single thread 
on each machine to run the client and server.

\begin{figure}[!t]
	\begin{center}
		\includegraphics[width=.6\columnwidth]{figures/testbed.png}
	\end{center}
	\vspace{-0.5cm}
	\caption{Performance Measurements Setup. C = Client. S = Server. R = 
	Router/Middlebox.}
	\label{fig:perf_testbed}
\end{figure}

In the emulated experiments of Sections~\ref{sec:bwaggr}, 
\ref{sec:eval_failover} and \ref{sec:app-migration} we use a Mininet 
network~\cite{handigol2012reproducible} composed of a client and a server, both 
dual-stacks. The IPv4 and IPv6 paths are completely disjoint in this emulated 
network, each offering 25Mbps and 10ms latency. 



%Our objective is to evaluate whether our design and implementation are indeed
%fast, flexible and does not conflict with several commercial and open-source
%middleboxes. Moreover, we expect to showcase and compare TCPLS's functionalities
%such as the App-level connection migration, the failover mechanism or the
%bandwidth aggregation capability. We discuss them against the state
%of the art designs, such as mvfst~\cite{mvfast}, quicly~\cite{quicly},
%msquic~\cite{msquic}, MPTCP~\ref{mptcp}, pquic~\cite{pquic},
%quic-go~\cite{quic-go} and MPQUIC~\ref{mpquic}.

%TODO do we also evelatuate security? with a simple proof and discussion?

%To evaluate \tcpls's functionalities, we rely on reproducible network
%experimentations with Mininet~\cite{mininet}. Our objective is to compare the
%behaviour of \tcpls with the state of the art, and to make it easily
%reproducible for future works, as the quic implementations continue to evolve.


\subsection{Raw Performance} \label{sec:perf}

With this first experiment, we demonstrate that \tcpls can be implemented at a
low cost compared to \tcp and \tls while 
The first question we want to answer is whether \tcpls can compete with the
traditional \tls over \tcp stack. We also compare our \tcpls prototype with
different \quic implementations.

\paragraph*{\tcpls}
For all the \tcpls measurements, we use a custom application that performs
large memory-to-memory transfers over a \tcpls session using a single stream.
\tcpls is configured to use the AES128-GCM-SHA256 cipher. Fig.~\ref{fig:perf}
provides the throughput as measured in our testbed. We report both the bandwidth in
gigabits per second and packets per second. Each bar in Fig.~\ref{fig:perf}
corresponds to the median measured over 10 seconds of stable throughput. The
bottom bar (labeled ``\tcpls tso on mtu 9000'') is the highest median throughput
that we measured with \tcpls: 12.4~Gbps. This result was obtained with jumbo
frames (i.e., 9000 bytes MTU) and using \tcp Segmentation Offload (TSO) on the
NICs. TSO is a standard feature that is enabled by all high-speed NICs. We
benchmark AES128-GCM-SHA256's decryption at
24.59~Gbps when doing in-memory decryption on our testbed's client (openssl
1.1.1i), and
13,62~Gbps when doing in-memory encryption on our testbed's server (openssl
1.1.1d). Note that on
both machines, encryption is $\approx 40\%$ slower than decryption.
which explains the gap between TCPLS and raw TCP.
The next bar (labeled ``\tcpls tso on'') shows that with TSO and standard frame
size (1,500 bytes MTU), the throughput still reaches 10.84 Gbps. These
results should be compared with the 22~Gbps \tcp reaches in the same
environment using \texttt{iperf} but without any encryption.



%the shows several interesting results. First, while offering similar (and more)
%capabilities than what QUIC is providing today,
%\tcpls is also more than twice faster than the strongest evaluated QUIC
%implementation (quicly) over CPU limited experimentations. We evaluate \tcpls

We then evaluate the impact of adding the \tcpls-level acknowledgments
described in Sec.~\ref{failover} to support failover.
% Thanks to these acknowledgments, \tcpls can support failover.
From a performance viewpoint, they increase the number of control records and
the number of system calls. Our prototype currently sends a \tcpls-ack for every
16 received records. Our measurements indicate that with this functionality,
\tcpls reaches 9.65 Gbps with a $1500$ bytes MTU.

%% We also compared \tcpls's native bandwidth aggregation using two streams with a
%% single stream produced by our \tcpls client on the top of the \mptcp kernel
%% creating two \mptcp subflows. Our \tcpls scheduler is a simple round-robin. The
%% \mptcp scheduler is the lowest-RTT scheduler enabled by default. Both
%% measurements are taken over the same 0.94.7 \mptcp kernel with a path MTU of
%% 1,500, in which \mptcp is disabled when the \tcpls native experiment is
%% performed.  Surprisingly, \tcpls's native bandwidth aggregation outperforms
%% \mptcp. Both experiments are CPU limited, and suffers from a raw throughput
%% degradation compared to the single-path experiment. \tcpls's reordering is
%% implemented with an heap~\cite{heap-github} highly efficient in
%% resizing, but the main factor holds on the number of packets treated for
%% reordering. Inside \mptcp, it reorders packets of $1,460$ bytes, even when those
%% packets are part of the \textit{same} record. This information is only available
%% at the \tcpls level, which reorders large chunk of data instead (max payload is
%% $16,384$ bytes). This is somehow similar to the $Jumbo$ frames optimization goal:
%% pushing larger data packets to reduce the number of times the same code is
%% called.  Besides
%% \tcpls's native  multipath uses the \tcp optimized stack, which together offer a
%% strong secure bandwidth aggregation solution.


Our next step was to evaluate the cost of the multi-connections stream steering feature of \tcpls.
We configured IPv4 and IPv6 addresses on the Client and the Server and created
a \tcpls session that uses two \tcp connections. Our Server steers the data in
the \tcpls streams using a
round-robin scheduler to send the records over the available connections. This
is not the best scheduler for sending data over multiple paths~\cite{paasch2014experimental} since it will force the
receiver to reorder the received records. \tcpls’s reordering is implemented
with an efficient heap pushing unordered records in $O(log(n))$
for $n$ elements in the heap, and retrieving ordered records
in $O(1)$. Our measurements show that when steering data in two streams
configured with bandwidth aggregation in our testbed,
\tcpls can reach a throughput of 8.8 Gbps, i.e. less than 10\% below the
failover mode.

\begin{figure}[!t]
  \begin{center}
    \includegraphics[width=\columnwidth]{figures/perf_analysis.png}
  \end{center}
\vspace{-0.5cm}
  \caption{\tcpls raw performance and comparison with state of the art.
    Single-thread client/server for all implementations.  The
    \tcpls prototype is faster than several major \quic implementations.}
  \label{fig:perf}
\end{figure}


%in four different settings: with a path mtu of 1500 or 9000, and with failover
%enabled or not. The failover functionality is an internal stack feature that
%provides session reliability, which increases the number of syscalls that TCPLS
%makes by exchanging TCPLS-level record acknowledgments. We currently send an
%ac%k for throughput at the price of a slower recovery in case of network
%failure.

\paragraph*{\tcpls vs. \tls/\tcp} We now compare the performance of \tcpls with
the traditional \tls over \tcp stack. To have a fair comparison, we use
\texttt{picotls}'s perf client/server implementation with the same commit as our
initial fork for \tcpls. Fig.~\ref{fig:perf} shows that \tls/\tcp provides a
lower performance than \tcpls with regular and jumbo frames. This can be
explained by the receiving buffer size provided to \texttt{read()}, which is
hardcoded to 16,384 bytes in \texttt{picotls}'s perf client. This implementation
choice results in fragmentation on the receiver and prevents it from using
efficiently the
zero-copy code path provided by the library. Our \tcpls implementation uses a
larger read buffer with auto-tuning
and benefits from zero-copy. At first, one may consider this as an unfair
comparison. \tcpls's design is an attempt to solve a common mistake. Indeed, in our \tcpls implementation,
the application developers cannot touch \texttt{read()}'s interface. This
implies that an application developer cannot misuse the relationship between
\tls and \tcp by creating fragmented records and doing unnecessary copies to
handle those fragments. \tcpls's design and implementation try to prevent
fragmentation by, first, having a sufficiently large read buffer size. Second,
\tcpls deciphers a record when it has received the entire content.
\tls libraries provide \tls/\tcp record fragmentation as a usability
feature that spares the application developers from taking into account all \tls
details, at the cost of lower performance. With \tcpls, application developers
can ignore \tls details without misusing the interface, which is why
this comparison is interesting. \tcpls constrains the application developer to
use the buffer type provided by the API, but offers zero-copy deciphered records
in return. Note, however, that the main culprit for the lower \tls/\tcp result
is the read buffer size. Increasing the read buffer size of the perf client
using \texttt{picotls} fills the performance gap with \tcpls.

%\todo{OB: pas sur que la suite est nécessaire} Moreover, but this has not
%yet been implemented, it could be interesting to match the TLS record size to
%the congestion window to deliver faster the data to the application when the
%network is congested.

\paragraph{\tcpls vs. \quic}
Although \quic~\cite{draft-ietf-quic-transport} is a young protocol, there are
already more than a dozen
implementations~\cite{marx2020same,quicimplem,yang2020making} under active
development. We compiled and installed three representative \quic
implementations in our testbed: Facebook's
\mvfst~\cite{mvfst-github,Joras_mvfst}, Microsoft's
\msquic~\cite{msquic-github}, and Fastly's \quicly~\cite{quicly-github}. They
are all developed by large companies that (plan to) use them in production and
include their own benchmarking application to perform throughput measurements.
Furthermore, \mvfst and \quicly support Generic Segmentation Offload (GSO), that
should improve performance by offloading \udp segmentation and checksum
computation to our NICs. We use the implementation's benchmarking application as
is, exploiting the optional arguments provided by their interface to increase
the throughput but drawing the line there. We do not modify the \quic
implementations and use their default cipher. Note, we tried to match the
transport parameters to the same ones used by \tcpls when they did not adversely
affect \quic's performance too much. For example, several congestion control
algorithms are implemented in the different \quic stacks but setting a different
one than the suggested default led to lower performance or unstable throughput
measurements.

The results in Fig.~\ref{fig:perf} show that \tcpls compares favorably
with the tested \quic implementations. The fastest \quic implementation is
\quicly. Thanks to GSO and with a 1450-byte \quic packet, it reaches 4.4~Gbps in our testbed with a 1,500 bytes
MTU. This result is directly comparable to \tcpls with TSO enabled, which still
performs more than twice faster at CPU limitation with similar configurations
for the receiving buffer size. Surprisingly, \quicly's performance decreases with
jumbo frames but is still faster than when GSO is disabled.  \msquic reaches
1.96~Gbps and \mvfst was slower despite GSO. The results are quite good for
\msquic and \quicly. Indeed, since \msquic does not yet use GSO, the speed of
this implementation is fundamentally limited by the speed of the Linux Kernel's
interface with UDP. \quic is known to have several performance shortcomings compared
to \tls/\tcp, in short: 1) \quic relies on $sendmsg$/$recvmsg$ which allows 1
packet at a time, 2) 1350-byte
plaintext encryptions (\quic) vs 16384-byte plaintext encryption for \tls and
\tcpls (higher is better),
3) Pacing in userspace for \quic, 4) Segmentation in the kernel for \quic using GSO not as good as TSO
(hardware, on the NIC for \tls/\tcp and \tcpls), 5) transport
acknowledgments in userspace for \quic increasing the number of context-switches. While some of these shortcomings may
still be improved, several are carved in \quic's choice to use UDP. There isn't likely enough room to match \tcpls performance
without offloading everything, which inherently breaks \quic's flexibility
argument and would be uncompliant regarding the end users' threat model (untrusted
network).


%QUIC is young protocol, and all implementations are in active development, with
%different states for their available features and optimizations. For example,
%none of the implementations were able to take advantage of a path mtu larger
%than 1500, and it even degraded the performance in the case of quicly. Two of them
%(quicly and mvfst) have implemented the support for \texttt{gso} leading in the
%case of quicly to more than 4 gb/s of throughput with a udp payload length
%configured to match TCP (1460).


%We perform a throughput evaluation of \tcpls and compare it to several major
%QUIC implementations: mvfst~\cite{} from Facebook, msquic~\cite{} from Microsoft
%and quicly~\cite{} from Fastly. Our choice of QUIC implementations was mainly
%influenced by the availability of a client/server perf tool specifically
%engineered for a throughput evaluation. A second criterion was the advancement
%of the implementation and the quality of the code. We hope to avoid most of the
%bugs negatively impacting their results by selecting the QUIC implementation
%that show advanced features and testings. A third criterion was the development
%language used. \tcpls is written in C, and we prefer to compare it against QUIC
%implementation written in a language compiled by clang or gcc. Mvfst, msquic
%and quicly meet these criteria.


\subsection{Middlebox Interference}
\label{sec:middlebox}

We have tested \tcpls against different opensource and commercial 
stateful
firewalls and proxy implementations (i.e., pfSense, IPFire, Cisco ASAv,
mitmproxy) and found no unexpected interference. Stateful filtering and stateful
packet inspection policies did not impact the \tcpls handshake, and transparent 
\tls
proxy successfully triggered \tcpls fallback to \tls/\tcp. Still, the security
appliances that block \tls 1.3 or some of its features
\cite{lee2019matls,Bock_China,raman2020measuring} would also block \tcpls.
%pervasive monitoring allows for configurable TLS extensions blocking
%\cite{rfc7258}. This is handled properly by \tcpls fallback mechanism.

When faced with middleboxes that modify \tls 
1.3~\cite{Bock_China,raman2020measuring}, only one type of \tcpls 
messages can be impacted. The client \tls extensions, that are 
integrity-protected but not confidential, can be modified by these middleboxes. 
They contain messages such as \hello, \join, \textit{SESSID} and 
\textit{COOKIE} presented 
in Sections~\ref{sec:tcpls-handshake} and \ref{sec:multipath}. Other messages 
conveyed in encrypted \tls records cannot be distinguished or tampered with.
When a client opens a \tcpls session
through a \tls terminating proxy not supportin \tcpls, it replies 
with 
a \textsc{ServerHello} message omitting the \hello extension. Then, the client 
can implicitly fall back to \tls and continue with the handshake.

Some legacy \tls server implementations do not implement the \tls
specification properly and might abort connections when receiving unknown \tls
extensions. Similar behavior has been observed in overly restrictive stateful
firewalls. To ensure connectivity in the presence of such policies, \tcpls
implements an explicit fallback mechanism. If a client receives a \tcp \rst in
response to the \tcpls \textsc{ClientHello}, or no response, it
tries negotiating a regular \tls connection, either
immediately similarly to Happy Eyeballs~\ref{TODO} or after a timeout. 
In the same manner, a \tcpls \join extension might be
blocked on a path when joining connections. In this case, the subflow 
attachment is canceled, and
the application is notified to react appropriately, e.g. to cancel the 
migration.

A more complete evaluation of middlebox interference would require measurements 
in various operational networks that include such 
devices~\cite{honda2011still,raman2020measuring,o2016tls}. This is
outside the scope of this paper and left for further work. 

%We tested \tcpls against opensource and commercial stateful firewalls and proxy
%implementations and (i.e., pfSense, IPFire, Cisco ASAv, mitmproxy) and found no
%interferences. Still, certain middlebox security appliances that implement
%pervasive monitoring allows for configurable TLS extensions blocking
%\cite{rfc7258}. This is handled properly by \tcpls fallback mechanism.


\subsection{Bandwidth Aggregation}
\label{sec:bwaggr}
We compare the \tcpls bandwidth aggregation capability to the state-of-the-art
\mptcp. 
%To this end, we build a Mininet network~\cite{handigol2012reproducible}
%composed of a client and a server, both dual-stacks. The IPv4 and IPv6 paths 
%are
%completely disjoint in this emulated network. Both protocols run in the same
%Mininet topology with two IP paths available, each offering 25Mbps and 10ms
%latency. 
Our experiment consists in transfering a 60MB file over a single path
and enabling the second one after $5$ seconds. \mptcp automatically detects
the new path once we enable the Client's second network interface. For \tcpls,
managing paths is different: the application can use the API to add local or
peer-related addresses at any point of the session. In this experiment, we send
this information at $5$ seconds, create a new \tcp connection to the peer, and
attach a new stream to it. The two streams are coupled together. Our results 
appear in Fig.~\ref{fig:multipath_aggregation}. Both protocols aggregate the 
two paths and reach a goodput of $\approx$ 50 Mbps. 

There are two major differences between
the two. First, for MPTCP, there is a delay before it becomes fully utilized. 
This delay is the time required by the Linux kernel
configures the new network interface IP address, adds the required routes, and
finally informs \mptcp~\cite{paasch2012exploring}.
%This explains the time that \mptcp needs to start to use the new path.
Second, \tcpls's aggregated goodput seems less stable than \mptcp. We explain 
this discrepancy by the difference of chunk size manipulated by the reordering 
algorithms in our experiment: \mptcp reorders packets with a payload of 1,460 
bytes, while \tcpls reorders records with the maximum 
payload size of 16,384 bytes. As concurrent network paths introduce reordering, 
which is reordered back by both \mptcp and \tcpls, a larger chunk size 
leads to larger goodput irregularities. 

%However, \tcpls might negotiate the record 
%size to smooth out this irregularity. 
Running the experiment with a record size 
of 1,500 bytes smooths out the irregularity at a slightly higher CPU cost since 
encryption and decryption are performed more often.
Appendix~\ref{appendix:aggr} shows the results of the same experiment but using
a \tls record size of 1,500 bytes instead.

\todo{Manque un takeaway qui cloture la section}
\todo{Quid des spikes qui restent?}

\begin{figure}[!t]
  \begin{center}
    \includegraphics[width=\columnwidth]{figures/aggregate_dual.png}
  \end{center}
\vspace{-0.5cm}
  \caption{Bandwidth aggregation comparison between \mptcp (top plot) and
    \tcpls (bottom plot) with a record payload size of 16,384 bytes.}
  \label{fig:multipath_aggregation}
\end{figure}

\subsection{Failover}
\label{sec:eval_failover}

Failover is designed to provide resilience to \tcpls connection
failures. Network outages may happen
for several reasons, e.g. middlebox interference or mobile clients losing
one access network (LTE or Wi-Fi). 
We first discuss and compare the recovery time
for different types of outages during a file transfer. We configure the client 
and the server with two interfaces, with one set to the 
backup mode in the case of \mptcp, i.e. no subflows are opened on the backup 
interface unless an issue is detected on the first interface. 
We consider three 
types of outages: a middlebox blackholing all the traffic, the reception of a 
spurious \rst, and the loss of the interface for \mptcp.
\todo{Why evaluate the loss of an interface if we can't compare it?}
Fig.~\ref{fig:recovery}
compares the goodput achieved by \mptcp and \tcpls over time when encountering 
these events.

\begin{figure}[!t]
  \begin{center}
    \includegraphics[width=.8\columnwidth]{figures/breakage_analysis.png}
  \end{center}
\vspace{-0.5cm}
  \caption{Recovery delays of \tcpls compared to \mptcp during a single outage.}
  \label{fig:recovery}
\end{figure}

We can observe that upon reception of a \tcp \rst, both
\tcpls and \mptcp react fast. This is an explicit signal they both act on 
quickly and resume the transfer. However, a network outage
is more difficult to detect. Both stacks rely on timers to decide to switch 
paths. In the case of \tcpls, we
configure a timer using the \tcp \texttt{User} \texttt{Timeout} 
option, which is set to $250~ms$ in our experiment.
%(mp): (should) already been explained in sec. prototype
%When the server enables failover, it sets a timeout to protect 
%the transfer. This option is sent in encrypted \tcpls records to 
%indicate to the client \tcpls's stack that it should not wait more than $X ms$ 
%between successful reads of data, with $X$ a parameter of this option. For 
%this experiment, the $USER\_TIMEOUT$ is set to $250~ms$.
When the timer expires, the client creates a connection over the other path and 
joins it to the session. The server then replays the unacknowledged records. 
Once these steps have succeeded, the transfer can continue. We can observe that 
this process take $\approx 1$ second to recover from this single outage in our 
experiment.


\begin{figure}[!t]
  \begin{center}
    \includegraphics[width=\columnwidth]{figures/tcpls_mptcp.png}
  \end{center}
\vspace{-0.5cm}
  \caption{\mptcp has difficulties to react to successive network outages. 
  \tcpls reacts quickly to such outages.}
  \label{fig:failover}
\end{figure}

We now extend our first experiment by adding periodic outages during the 
transfer in a 4-path network topology.
%The second experiment involves observing how different multipath
%implementations behave when several failures occur during a data transfer.
We tested several multipath transport implementations: \mptcp, PQUIC with the 
multipath
plugin~\cite{de2019pluginizing}, and MPQUIC~\cite{de2017multipath}.
We do not report the results with PQUIC and MPQUIC as 
detecting and recovering from network failures were not part of these 
prototypes.
%We dug into these implementations and observed that they were not programmed 
%to 
%detect network failures and correctly switch the network path, thus we 
%preferred to ignore those results. 
\mptcp is a more mature multipath transport implementation that is maintained 
and used in production. Fig.~\ref{fig:failover} compares how 
\mptcp and \tcpls recovers when three paths out of four are blackholed every 
five seconds. We rotate the working path so that each implementation has 
first to find it before recovering the transfer. 
We observe that \mptcp's default path manager performs well on the first
failure. For the next ones, it requires several seconds to recover the right 
path. 
%Besides, \mptcp seems to remember that something wrong
%happened, and enters a kind of congestion avoidance state until the end of the
%transfer. We may question this design choice, since many \tcp connections on 
%the
%Internet have a shorter lifespan than 10 seconds (i.e., the time after which
%\mptcp should re-use a previously used path in this experiment). 
We also tested injecting \tcp \rst instead of blackholing paths and \mptcp 
indefinitely stalled after the second \rst.
However, \tcpls finds the right path quicker and recovers the session in a 
short time similarly to the drop and \rst outage studied in 
Fig.~\ref{fig:recovery}. Moreover, since those connections are fresh, they can 
quickly use the available bandwidth.

\subsection{Application-Level Migration}
\label{sec:app-migration}

When there are multiple IP paths, connection migration might be a
powerful tool to improve the application's resilience to network problems. Applications can take advantage of the \tcpls API to migrate their traffic from one network path to another. Fig.~\ref{fig:conn_migration} shows the result of an Application-level connection migration demo using the API (i.e., the application decides when to migrate, and we expose a simplistic code flow to perform it). In this experiment, we reuse the Mininet topology introduced in Sec.~\ref{sec:bwaggr}. We set a bandwidth of 30Mbps on each path, a 40ms round-trip-time for the v4 link, and 80ms for the v6 link. Our application downloads
a 60MB file from a server and migrates twice (i.e., from the IPv4 to the IPv6 path and once again to the IPv4 path).

Triggering the first connection migration involves chaining five API calls: first, if a \tcp connection is not already established, the application issues a
\texttt{tcpls\_connect()} that creates a $\tcp$ connection (or directly uses
\texttt{tcpls\_handshake()} with TFO enabled. Calling \texttt{tcpls\_handshake()} is required and needs to be configured with handshake properties announcing a \join over the IPv6 connection. Then, the creation of a new stream \texttt{tcpls\_stream\_new()} for the IPv6 connection, finally followed by the attachment of this new stream \texttt{tcpls\_streams\_attach()} and the secure closing of the IPv4 \tcp connection using \texttt{tcpls\_stream\_close()}. Following these events, the server seamlessly switches the path while looping over \texttt{tcpls\_send()} to send the file. Note that all the events trigger callbacks on the server side, to let the server react appropriately if other requirements need to be fulfilled.

Fig.~\ref{fig:conn_migration} shows a peak in goodput during each migration. During the migration window (marked with vertical black bars indicating the attachment of the new stream and the closing of the initial stream), the client is taking advantage of both paths to receive its data: the first path finishes to flush its data while the second path is already used. Once the last record of the first path is sent, \tcpls can reorder the data and deliver it to the client. We then obtain a goodput peak matching the accumulated bandwidth over both paths during the migration time window.

\begin{figure}[!t]
  \begin{center}
    \includegraphics[width=.8\columnwidth]{figures/migration.png}
  \end{center}
\vspace{-0.5cm}
  \caption{Application-level connection migration during a 60MB file download.
    \tcpls temporarily aggregates the two network paths during such a migration.}
  \label{fig:conn_migration}
\end{figure}

\subsection{Dynamically Extending \tcpls}

The \tcpls streams enable new use cases. A \tcpls application can
create and use different streams to carry data. However, since these streams
are generic, they can also be used by the \tcpls implementation itself to
exchange control information. To demonstrate the versatility of these control
streams, we extended \tcpls to enable a server to push a different congestion
control scheme to a specific client over an existing \tcpls session. Recent
work on restructuring congestion control has proposed a generic architecture
for congestion controllers~\cite{narayan2018restructuring}.
During the last years, the Linux kernel developers have relied on eBPF
to make the Linux TCP/IP stack~\cite{brakmo2017tcp,tran2020beyond} easier
to extend. Since Linux kernel version 5.6, an application can inject
a different congestion control scheme entirely implemented using eBPF. A similar
approach was proposed for \quic in Pluginizing \quic~\cite{de2019pluginizing}. We
leverage these new eBPF capabilities to demonstrate the feasibility of injecting
and updating a congestion control scheme during a \tcpls session.

We perform our experiment using Mininet over a 100Mbps emulated link that has a
60msec delay. Fig.~\ref{fig:vegasCubic} shows a client that uses the \tcp
Vegas~\cite{10.1145/190314.190317} congestion control scheme to upload a file.
This \tcpls session fully uses the bottleneck link. After some time, another
client starts an upload, but using the CUBIC congestion
controller~\cite{rfc8312}. This results in an unfair distribution of the
bandwidth. The server then sends the eBPF bytecode implementing the CUBIC
congestion control scheme to the \tcp Vegas client that injects it in its kernel
and the unfairness disappears. We performed the same experiment for different
delays, varying from 10ms to 100ms and obtained similar results.

\begin{figure}[!t]
  \begin{center}
    \includegraphics[width=.8\columnwidth]{pretty_plotify/plots/vegas_cubic.png}
  \end{center}
\vspace{-0.5cm}
  \caption{\tcpls hosts can exchange congestion control schemes compiled into eBPF bytecode and activate them during a \tcpls session.}
  \label{fig:vegasCubic}
\end{figure}
